<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Code Analysis Systems - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "tk53e05gf2");
    </script>
    
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-space">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/danwahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark-white.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">The Starship Repository Explorer</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Adventure</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 4: Code Analysis Systems</h1>
<hr>
<p>As you venture deeper into the Repository Explorer, you arrive at the Code Analysis Systems, the ship&#39;s intelligence hub. Here, the <code class="inline-code">RepoAnalyzer</code> and <code class="inline-code">LLMClient</code> modules work in tandem to extract insights from codebases and generate interactive adventures. The analyzer acts as a cosmic scanner, mapping the codebase&#39;s structure, while the LLM serves as the ship&#39;s oracle, transforming raw data into engaging narratives. Together, they ensure the starship can navigate even the densest nebulae of code with precision and creativity.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Content Optimization</strong>: How targeted analysis reduces token usage and focuses on critical code elements.</li>
<li>üîç <strong>LLM Integration</strong>: How the <code class="inline-code">LLMClient</code> communicates with AI models, including adaptive throttling and rate limit handling.</li>
<li>‚ö° <strong>Caching Strategies</strong>: How caching improves performance and avoids redundant computations in code analysis.</li>
<li>üí° <strong>Error Handling Patterns</strong>: Techniques for managing rate limits and ensuring graceful degradation in AI-powered systems.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">packages/core/src/analyzer/repo-analyzer.ts</code></a></h3>
<p>The <code class="inline-code">RepoAnalyzer</code> module is the ship&#39;s cosmic scanner, tasked with analyzing codebases and extracting meaningful insights. It uses Repomix subprocesses to generate structured data, applies optimization techniques to reduce token usage, and validates project paths for security. This file demonstrates advanced caching strategies and function-focused content extraction to ensure efficient and secure analysis.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L296" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateRepomixContext</code></a> scans the entire codebase or specific files, applying compression and caching to reduce resource usage.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L246" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateTargetedContent</code></a> focuses on specific files, optimizing content for token efficiency while validating paths.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L610" target="_blank" rel="noopener noreferrer"><code class="inline-code">captureRepomixStdout</code></a> uses subprocesses to execute Repomix, with timeout and memory protection mechanisms.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/shared/input-validator.ts#L37" target="_blank" rel="noopener noreferrer"><code class="inline-code">validateProjectPath</code></a> ensures paths are safe and normalized, preventing security vulnerabilities.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L368" target="_blank" rel="noopener noreferrer"><code class="inline-code">extractFunctionFocusedContent</code></a> minimizes token usage by extracting only essential functions and patterns.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">async generateRepomixContext(projectPath: string, options: RepomixOptions = {}): Promise&lt;string&gt; {
  this.validateProjectPath(projectPath);
  const configuredFiles = extractUniqueFilePaths(projectPath);

  if (configuredFiles.length &gt; 0) {
    try {
      return await this.generateOptimizedContent(projectPath, configuredFiles);
    } catch (error) {
      console.warn(`Failed to generate optimized content, falling back: ${error.message}`);
      return await this.generateTargetedContent(projectPath, configuredFiles);
    }
  }
  
  const cacheKey = `${path.resolve(projectPath)}:${JSON.stringify(options)}`;
  const cached = this.cache.get(cacheKey);
  if (cached &amp;&amp; (Date.now() - cached.timestamp) &lt; REPOMIX_CACHE_TTL) {
    return cached.content;
  }
  
  const cliOptions: CliOptions = { style: options.style || &#39;markdown&#39;, compress: options.compress !== false };
  const context = await this.captureRepomixStdout([&#39;.&#39;], projectPath, cliOptions);
  this.cache.set(cacheKey, { content: context, timestamp: Date.now() });
  return context;
}
</code></pre>
<ul>
<li>This function dynamically switches between optimized, targeted, or full analysis based on project configuration.</li>
<li>Caching reduces redundant computations, improving performance for repeated scans.</li>
<li>The fallback mechanism ensures robustness by gracefully degrading to simpler methods when errors occur.</li>
<li>Compression options reduce token usage, making the analysis more efficient for LLM processing.</li>
<li>Path validation prevents unauthorized access and ensures secure file handling.</li>
</ul>
<hr>
<pre><code class="language-typescript">private async captureRepomixStdout(directories: string[], cwd: string, options: CliOptions): Promise&lt;string&gt; {
  return new Promise((resolve, reject) =&gt; {
    const args = [...directories, &#39;--stdout&#39;, &#39;--style&#39;, options.style || &#39;markdown&#39;];
    if (options.compress) args.push(&#39;--compress&#39;);
    const repomix = spawn(&#39;npx&#39;, [&#39;repomix&#39;, ...args], { cwd, stdio: [&#39;pipe&#39;, &#39;pipe&#39;, &#39;pipe&#39;] });

    let stdout = &#39;&#39;;
    let stderr = &#39;&#39;;
    repomix.stdout.on(&#39;data&#39;, (data) =&gt; { stdout += data.toString(); });
    repomix.stderr.on(&#39;data&#39;, (data) =&gt; { stderr += data.toString(); });
    repomix.on(&#39;close&#39;, (code) =&gt; {
      if (code === 0) resolve(stdout.trim());
      else reject(new Error(`Repomix failed with code ${code}: ${stderr.trim()}`));
    });
  });
}
</code></pre>
<ul>
<li>This subprocess execution captures Repomix output, enabling dynamic codebase analysis.</li>
<li>Timeout and memory protection mechanisms prevent resource exhaustion during execution.</li>
<li>Arguments are dynamically constructed based on user options, providing flexibility in analysis style.</li>
<li>The use of Promises ensures asynchronous handling, avoiding blocking operations in the application.</li>
<li>Error handling captures subprocess failures and provides detailed feedback for debugging.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">packages/core/src/llm/llm-client.ts</code></a></h3>
<p>The <code class="inline-code">LLMClient</code> module serves as the ship&#39;s oracle, interfacing with AI models to generate responses based on analyzed data. It features adaptive throttling, rate limit detection, and response validation to ensure reliable communication with AI providers.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L140" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateResponse</code></a> sends prompts to the LLM, handles rate limits, and validates responses for accuracy.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L87" target="_blank" rel="noopener noreferrer"><code class="inline-code">constructor</code></a> initializes the client, detecting the appropriate API provider and setting up configurations.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L113" target="_blank" rel="noopener noreferrer"><code class="inline-code">getApiKey</code></a> retrieves the correct API key based on the provider, ensuring secure authentication.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L128" target="_blank" rel="noopener noreferrer"><code class="inline-code">isAzureOpenAI</code></a> checks whether the provider is Azure OpenAI, enabling specific configurations.</li>
<li><a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L392" target="_blank" rel="noopener noreferrer"><code class="inline-code">detectRateLimitType</code></a> identifies rate limit errors and provides actionable feedback.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">async generateResponse(prompt: string, options?: LLMRequestOptions): Promise&lt;LLMResponse&gt; {
  await this.applyThrottling();
  try {
    const requestParams = this.buildRequestParams(prompt, options);
    const completion = await this.executeRequest(requestParams);
    let content = this.validateResponse(completion);

    if (options?.responseFormat === &#39;json_object&#39;) {
      content = this.cleanJsonResponse(content);
    }
    this.logTokenUsage(completion, options?.context);
    this.onSuccessfulRequest();

    return { content };
  } catch (error) {
    const rateLimitInfo = this.detectRateLimitType(error);
    if (rateLimitInfo.type !== RateLimitType.NONE) {
      this.activateThrottling(error);
      throw new RateLimitError(rateLimitInfo.type, rateLimitInfo.waitSeconds, rateLimitInfo.message, error);
    }
    throw new Error(`LLM request failed: ${error.message}`);
  }
}
</code></pre>
<ul>
<li>This function integrates adaptive throttling to manage rate limits dynamically.</li>
<li>Response validation ensures the output is accurate and actionable, with fallback mechanisms for errors.</li>
<li>JSON cleaning improves usability by removing unnecessary wrappers around structured data.</li>
<li>Logging provides transparency on token usage, helping developers monitor costs and efficiency.</li>
<li>Rate limit error handling ensures graceful degradation, avoiding abrupt failures.</li>
</ul>
<hr>
<pre><code class="language-typescript">constructor() {
  this.model = LLM_MODEL;
  const apiKey = this.getApiKey();
  if (!apiKey || !LLM_BASE_URL) {
    throw new Error(&#39;LLM configuration required.&#39;);
  }
  if (this.isAzureOpenAI()) {
    const azureEndpoint = LLM_BASE_URL.split(&#39;/openai/deployments&#39;)[0];
    this.client = new AzureOpenAI({ endpoint: azureEndpoint, apiKey, apiVersion: LLM_API_VERSION, deployment: this.model });
  } else {
    this.client = new OpenAI({ apiKey, baseURL: LLM_BASE_URL });
  }
}
</code></pre>
<ul>
<li>The constructor initializes the client based on provider-specific configurations, supporting both OpenAI and Azure OpenAI.</li>
<li>Environment variables are used to securely retrieve API keys and endpoints.</li>
<li>Modular design allows seamless switching between different AI providers without code changes.</li>
<li>Deployment-specific configurations ensure compatibility with various models and endpoints.</li>
<li>Error handling prevents initialization when critical configurations are missing.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Study how <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L296" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateRepomixContext</code></a> uses caching to improve performance and reduce redundant processing.</li>
<li>Observe how <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L610" target="_blank" rel="noopener noreferrer"><code class="inline-code">captureRepomixStdout</code></a> handles subprocess execution securely, avoiding common pitfalls like memory leaks.</li>
<li>Explore how <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L140" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateResponse</code></a> integrates adaptive throttling to manage rate limits dynamically.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><strong>Optimize Analysis Pipeline</strong>: Modify <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/analyzer/repo-analyzer.ts#L246" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateTargetedContent</code></a> to include additional filtering for specific file types (e.g., <code class="inline-code">.ts</code> files only). Test the performance impact on large codebases.</li>
<li><strong>Trace LLM Requests</strong>: Add logging to <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L140" target="_blank" rel="noopener noreferrer"><code class="inline-code">generateResponse</code></a> to capture the complete flow of a prompt from submission to response. Analyze how throttling affects request timing.</li>
<li><strong>Improve Error Handling</strong>: Enhance <a href="https://github.com/danwahlin/ai-repo-adventures/blob/main/packages/core/src/llm/llm-client.ts#L392" target="_blank" rel="noopener noreferrer"><code class="inline-code">detectRateLimitType</code></a> to provide more detailed error messages, including suggestions for upgrading API tiers or reducing request frequency.</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the starship&#39;s systems.</p>
<p>Mission accomplished, Code Analysis Systems explorers‚Äîyour cosmic coding skills have rocketed to stellar heights, propelling you 60% through the galaxy of quests! ‚≠êüöÄüì°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-3.html" class="prev-quest-btn">‚Üê Previous: Quest 3</a>
        <a href="quest-5.html" class="next-quest-btn">Next: Quest 5 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>