<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quest 3: Power the Analysis Array - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="assets/quest-navigator.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }
        
        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body>

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="index.html">Spacebound Repository Odyssey</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Theme</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <a href="https://github.com/danwahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="assets/images/github-mark-white.svg" alt="GitHub" width="24" height="24">
                </a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 3: Power the Analysis Array</h1>
<hr>
<p>Your mission systems are ready to convert repositories into navigational context. Engineering requests a precise calibration of the Analysis Array, which binds the LLM reactor to the repository scanner. This quest focuses on the analysis pipeline that compresses code and executes LLM calls under strict time controls. Use the bridge console‚Äôs instrumentation to verify initialization, safety checks, and subprocess orchestration. The exploration guides below are prompts you will answer by reading the provided code excerpts, not prerequisites you must fulfill in advance.</p>
<h2>Quest Objectives</h2>
<p>As you explore the code below, investigate these key questions:</p>
<ul>
<li>üîç Scanner Calibration: How does <code class="inline-code">validateProjectPath</code> restrict unsafe or malformed project paths, and what specific checks prevent invalid input?</li>
<li>‚ö° Reactor Synchronization: In <code class="inline-code">generateTargetedContent</code>, how are CLI options constructed and cached to minimize redundant subprocess runs?</li>
<li>üõ°Ô∏è Timeout Shielding: How does <code class="inline-code">captureRepomixStdout</code> implement both graceful and forceful termination, and what memory guardrails are enforced during streaming?</li>
<li>üîç Model Routing Logic: How does <code class="inline-code">LLMClient.getApiKey</code> decide between <code class="inline-code">GITHUB_TOKEN</code> and <code class="inline-code">LLM_API_KEY</code>, and what errors are surfaced when values are missing?</li>
<li>üõ°Ô∏è Provider Safety: What conditions trigger Azure-specific client setup in <code class="inline-code">LLMClient.constructor</code>, and how does <code class="inline-code">isAzureOpenAI</code> influence initialization?</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">packages/core/src/analyzer/repo-analyzer.ts:</span> Repository context extraction and optimization pipeline</h3>
<p>This module implements a secure, cache-aware wrapper around the <code class="inline-code">repomix</code> CLI to generate repository context. It provides targeted extraction for specific files and optimized content for function-focused summaries. The <code class="inline-code">validateProjectPath</code> method enforces basic input hygiene, rejecting empty strings and null-byte injection. The <code class="inline-code">generateTargetedContent</code> method coordinates deduplication, normalization, and inclusion construction before triggering a subprocess. It applies cache keys based on resolved paths, normalized file lists, and compression flags to avoid redundant work. The <code class="inline-code">captureRepomixStdout</code> method executes <code class="inline-code">repomix</code> with explicit flags for compression, comment removal, and no directory structure, then streams output with size limits to protect memory. It uses a layered timeout strategy: a primary timeout that sends <code class="inline-code">SIGTERM</code>, followed by a grace period and <code class="inline-code">SIGKILL</code> fallback. Exit codes and stderr are captured to produce concise failure messages. Together, these functions provide a reliable foundation for generating compact, targeted context while protecting system resources, controlling latency, and maintaining deterministic cache behavior. The focus is on security boundaries, stable keys, and operational resilience under varying repository sizes and content structures.</p>
<h4>Highlights</h4>
<ul>
<li><code class="inline-code">generateRepomixContext</code> orchestrates full or configured analysis with caching, ignore patterns, and compression defaults to produce consistent repository context efficiently.</li>
<li><code class="inline-code">generateTargetedContent</code> validates and normalizes specific file paths, builds precise <code class="inline-code">repomix</code> CLI options, applies caching, and executes a bounded subprocess run.</li>
<li><code class="inline-code">captureRepomixStdout</code> spawns <code class="inline-code">repomix</code>, streams stdout with buffer limits, applies timeouts with graceful and forced termination, and surfaces actionable error messages.</li>
</ul>
<h2>Code</h2>
<h3>packages/core/src/analyzer/repo-analyzer.ts</h3>
<pre><code class="language-typescript">async generateRepomixContext(projectPath: string, options: RepomixOptions = {}): Promise&lt;string&gt; {
  this.validateProjectPath(projectPath);
  
  // Check if adventure.config.json has specific files to include
  const configuredFiles = extractUniqueFilePaths(projectPath);
  
  if (configuredFiles.length &gt; 0) {
    // Use configured files with optimized content generation
    console.log(`Using adventure.config.json: analyzing ${configuredFiles.length} configured files with optimization`);
    try {
      return await this.generateOptimizedContent(projectPath, configuredFiles);
    } catch (error) {
      console.warn(`Failed to generate optimized content, falling back to targeted content: ${error instanceof Error ? error.message : String(error)}`);
      try {
        return await this.generateTargetedContent(projectPath, configuredFiles);
      } catch (fallbackError) {
        console.warn(`Failed to generate targeted content, falling back to full repomix content: ${fallbackError instanceof Error ? fallbackError.message : String(fallbackError)}`);
        // Fall through to full content generation
      }
    }
  }
  
  // Fallback: use existing behavior with all files (compressed)
  console.log(&#39;No adventure.config.json found: analyzing full codebase (compressed)&#39;);
  
  // Create cache key from path and options
  const cacheKey = `${path.resolve(projectPath)}:${JSON.stringify(options)}`;
  
  // Check cache first
  const cached = this.cache.get(cacheKey);
  if (cached &amp;&amp; (Date.now() - cached.timestamp) &lt; REPOMIX_CACHE_TTL) {
    return cached.content;
  }
  
  
  try {
    // Build ignore patterns
    const ignorePatterns = [
      &#39;node_modules&#39;, &#39;dist&#39;, &#39;build&#39;, &#39;.git&#39;, &#39;coverage&#39;, &#39;.nyc_output&#39;
    ];
    
    if (!options.includeTests) {
      ignorePatterns.push(&#39;**/*.test.ts&#39;, &#39;**/*.spec.ts&#39;, &#39;**/tests/**&#39;, &#39;**/test/**&#39;);
    }

    // Configure repomix options
    const cliOptions: CliOptions = {
      style: options.style || &#39;markdown&#39;,
      stdout: true,
      compress: options.compress !== false, // Default to true unless explicitly disabled
      ignore: ignorePatterns.join(&#39;,&#39;),
      removeComments: true,
      removeEmptyLines: true,
      noDirectoryStructure: true
    };

    // Capture stdout during repomix execution
    const context = await this.captureRepomixStdout([&#39;.&#39;], projectPath, cliOptions);
    
    // Cache the result
    this.cache.set(cacheKey, { content: context, timestamp: Date.now() });
    
    return context;
  } catch (error) {
    throw new Error(`Repomix execution failed: ${error instanceof Error ? error.message : String(error)}`);
  }
}
</code></pre>
<ul>
<li>Coordinates top-level analysis with a three-tiered fallback: optimized configured files, targeted extraction, then full compressed output.</li>
<li>Uses cache keys that include absolute path and serialized options to ensure deterministic reuse.</li>
<li>Applies ignore patterns to reduce noise and token load while preserving essential code.</li>
<li>Centralizes subprocess execution via <code class="inline-code">captureRepomixStdout</code>, simplifying error and timeout handling.</li>
<li>Demonstrates composable configuration through the <code class="inline-code">CliOptions</code> object.</li>
</ul>
<hr>
<pre><code class="language-typescript">async generateTargetedContent(projectPath: string, targetFiles: string[], compress: boolean = true): Promise&lt;string&gt; {
  this.validateProjectPath(projectPath);
  
  if (!targetFiles || targetFiles.length === 0) {
    throw new Error(&#39;Target files array cannot be empty&#39;);
  }
  
  // Harden and validate target files
  const safeFiles = this.validateAndNormalizeTargetFiles(projectPath, targetFiles);
  if (safeFiles.length === 0) {
    throw new Error(&#39;No valid target files found after validation&#39;);
  }
  
  // Create stable cache key from normalized files
  const cacheKey = `${path.resolve(projectPath)}:targeted:${safeFiles.join(&#39;,&#39;)}:compress=${compress}`;
  
  // Check cache first
  const cached = this.cache.get(cacheKey);
  if (cached &amp;&amp; (Date.now() - cached.timestamp) &lt; REPOMIX_CACHE_TTL) {
    return cached.content;
  }
  
  
  try {
    // Configure repomix options for targeted extraction
    const cliOptions: CliOptions = {
      style: &#39;markdown&#39;,
      stdout: true,
      compress: compress, // Configurable compression
      include: safeFiles.join(&#39;,&#39;), // Only include validated files
      removeComments: compress, // Remove comments if compressing
      removeEmptyLines: compress, // Remove empty lines if compressing
      noDirectoryStructure: true
    };

    // Capture stdout during repomix execution
    const context = await this.captureRepomixStdout([&#39;.&#39;], projectPath, cliOptions);
    
    // Cache the result
    this.cache.set(cacheKey, { content: context, timestamp: Date.now() });
    
    return context;
  } catch (error) {
    throw new Error(`Targeted repomix execution failed: ${error instanceof Error ? error.message : String(error)}`);
  }
}
</code></pre>
<ul>
<li>Validates inputs and normalizes the file set to prevent path traversal and non-existent references.</li>
<li>Builds a stable cache key using normalized file order and compression flag to avoid redundant analysis.</li>
<li>Constructs <code class="inline-code">repomix</code> CLI options that explicitly include only vetted files, minimizing output size.</li>
<li>Ties compression toggles to comment and empty-line removal for consistent reduction behavior.</li>
<li>Delegates process execution to a single utility that encapsulates timeout and memory safety.</li>
</ul>
<hr>
<pre><code class="language-typescript">private async captureRepomixStdout(directories: string[], cwd: string, options: CliOptions): Promise&lt;string&gt; {
  return new Promise((resolve, reject) =&gt; {
    // Build repomix CLI arguments
    const args = [
      ...directories,
      &#39;--stdout&#39;,
      &#39;--style&#39;, options.style || &#39;markdown&#39;
    ];
    
    if (options.compress) args.push(&#39;--compress&#39;);
    if (options.removeComments) args.push(&#39;--remove-comments&#39;);
    if (options.removeEmptyLines) args.push(&#39;--remove-empty-lines&#39;);
    if (options.noDirectoryStructure) args.push(&#39;--no-directory-structure&#39;);
    if (options.ignore) args.push(&#39;--ignore&#39;, options.ignore);
    if (options.include) args.push(&#39;--include&#39;, options.include);
    
    // Spawn repomix as subprocess
    const repomix = spawn(&#39;npx&#39;, [&#39;repomix&#39;, ...args], {
      cwd,
      stdio: [&#39;pipe&#39;, &#39;pipe&#39;, &#39;pipe&#39;]
    });
    
    let stdout = &#39;&#39;;
    let stderr = &#39;&#39;;
    let stdoutSize = 0;
    let isResolved = false;
    
    // Set up timeout with graceful then force kill
    const timeout = setTimeout(() =&gt; {
      if (!isResolved) {
        console.warn(`Repomix subprocess timeout (${REPOMIX_SUBPROCESS_TIMEOUT}ms), killing process...`);
        repomix.kill(&#39;SIGTERM&#39;);
        setTimeout(() =&gt; repomix.kill(&#39;SIGKILL&#39;), REPOMIX_GRACEFUL_TIMEOUT);
        isResolved = true;
        reject(new Error(`Repomix subprocess timed out after ${REPOMIX_SUBPROCESS_TIMEOUT}ms (${cwd})`));
      }
    }, REPOMIX_SUBPROCESS_TIMEOUT);
    
    repomix.stdout.on(&#39;data&#39;, (data) =&gt; {
      const chunk = data.toString();
      stdoutSize += chunk.length;
      
      // Memory protection
      if (stdoutSize &gt; REPOMIX_MAX_BUFFER_SIZE) {
        if (!isResolved) {
          isResolved = true;
          clearTimeout(timeout);
          repomix.kill(&#39;SIGKILL&#39;);
          reject(new Error(`Repomix output too large (${stdoutSize} bytes) for ${cwd}`));
        }
        return;
      }
      
      stdout += chunk;
    });
    
    repomix.stderr.on(&#39;data&#39;, (data) =&gt; {
      stderr += data.toString();
    });
    
    repomix.on(&#39;close&#39;, (code) =&gt; {
      if (!isResolved) {
        isResolved = true;
        clearTimeout(timeout);
        
        if (code === 0 &amp;&amp; stdout.trim().length &gt; 0) {
          resolve(stdout);
        } else {
          reject(new Error(`Repomix failed (exit ${code}): ${stderr.substring(0, 200)}`));
        }
      }
    });
    
    repomix.on(&#39;error&#39;, (error) =&gt; {
      if (!isResolved) {
        isResolved = true;
        clearTimeout(timeout);
        reject(new Error(`Repomix spawn failed: ${error.message}`));
      }
    });
  });
}
</code></pre>
<ul>
<li>Encapsulates subprocess lifecycle with explicit argument construction based on provided options.</li>
<li>Implements dual-stage termination: <code class="inline-code">SIGTERM</code> followed by a delayed <code class="inline-code">SIGKILL</code> to prevent zombie processes.</li>
<li>Streams stdout incrementally, enforcing <code class="inline-code">REPOMIX_MAX_BUFFER_SIZE</code> to guard memory.</li>
<li>Provides concise, actionable error messages using exit codes and truncated stderr for readability.</li>
<li>Centralizes timeout and error handling to keep higher-level functions clean and focused.</li>
</ul>
<hr>
<h3>packages/core/src/llm/llm-client.ts</h3>
<pre><code class="language-typescript">constructor() {
  this.model = LLM_MODEL;
  
  // Determine API key based on provider
  const apiKey = this.getApiKey();
  if (!apiKey || !LLM_BASE_URL) {
    throw new Error(&#39;LLM configuration required. Please set LLM_BASE_URL and appropriate API key (LLM_API_KEY or GITHUB_TOKEN).&#39;);
  }

  if (this.isAzureOpenAI()) {
    // Azure OpenAI requires endpoint without the path
    const azureEndpoint = LLM_BASE_URL.split(&#39;/openai/deployments&#39;)[0];
    this.client = new AzureOpenAI({
      endpoint: azureEndpoint,
      apiKey,
      apiVersion: LLM_API_VERSION,
      deployment: this.model
    });
  } else {
    this.client = new OpenAI({
      apiKey,
      baseURL: LLM_BASE_URL
    });
  }
}
</code></pre>
<ul>
<li>Initializes the client with model selection and validates presence of base URL and API key.</li>
<li>Chooses provider-specific client class and configuration based on environment.</li>
<li>Handles Azure endpoint normalization by stripping path segments to the root service URL.</li>
<li>Uses deployment-based configuration for Azure and base URL for standard OpenAI-compatible services.</li>
<li>Surfaces clear configuration errors early to prevent runtime ambiguity.</li>
</ul>
<hr>
<pre><code class="language-typescript">private getApiKey(): string {
  // GitHub Models (hosted on Azure) uses GITHUB_TOKEN
  if (LLM_BASE_URL.includes(&#39;models.inference.ai.azure.com&#39;)) {
    if (!GITHUB_TOKEN) {
      throw new Error(&#39;GITHUB_TOKEN required for GitHub Models. Set GITHUB_TOKEN environment variable.&#39;);
    }
    return GITHUB_TOKEN;
  }
  // All other providers (OpenAI, Azure OpenAI, Ollama, etc.) use LLM_API_KEY
  if (!LLM_API_KEY) {
    throw new Error(&#39;LLM_API_KEY required. Set LLM_API_KEY environment variable.&#39;);
  }
  return LLM_API_KEY;
}
</code></pre>
<ul>
<li>Implements provider-aware credential routing driven by the base URL.</li>
<li>Enforces required tokens per provider with explicit error messages for fast diagnosis.</li>
<li>Avoids silent fallbacks, ensuring misconfigurations are discovered during initialization.</li>
<li>Keeps logic simple and deterministic, anchored to environment variables.</li>
<li>Supports both GitHub Models and general OpenAI-compatible providers.</li>
</ul>
<hr>
<pre><code class="language-typescript">private isAzureOpenAI(): boolean {
  return LLM_BASE_URL.includes(&#39;.openai.azure.com&#39;) || LLM_BASE_URL.includes(&#39;cognitiveservices.azure.com&#39;);
}
</code></pre>
<ul>
<li>Detects Azure OpenAI by matching known host patterns.</li>
<li>Drives constructor branching to select <code class="inline-code">AzureOpenAI</code> client and appropriate parameters.</li>
<li>Maintains separation of concerns by isolating provider detection logic.</li>
<li>Reduces duplication and prevents misapplication of standard OpenAI settings to Azure endpoints.</li>
<li>Keeps checks straightforward for maintainability.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Validate paths early: use <code class="inline-code">validateProjectPath</code> before any file operations or subprocess calls.</li>
<li>For faster iterations, prefer <code class="inline-code">generateTargetedContent</code> with a minimal <code class="inline-code">include</code> set and compression enabled.</li>
<li>Confirm LLM environment: ensure <code class="inline-code">LLM_BASE_URL</code> aligns with your provider, and set the correct token variable.</li>
</ul>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries.</p>
<p>Quest 3: Power the Analysis Array successfully executed‚Äîyour pipeline now compiles cleanly, passes all integration tests, and scales to spec, elevating system observability and moving you to 40% completion; outstanding work shipping this milestone to prod with crisp commit hygiene and resilient architecture patterns ‚ö°üöÄ</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-2.html" class="prev-quest-btn">‚Üê Previous: Quest 2</a>
        <a href="quest-4.html" class="next-quest-btn">Next: Quest 4 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="assets/quest-navigator.js"></script>
</body>
</html>