# LLM Configuration - Choose your provider

# === GitHub Models (Free tier available) ===
GITHUB_TOKEN=your_github_token_here
LLM_BASE_URL=https://models.inference.ai.azure.com
LLM_MODEL=gpt-4o-mini
# Available models: gpt-4o-mini, gpt-4o, claude-3-5-sonnet, llama-3-3-70b

# === OpenAI API (Direct) ===
# OPENAI_API_KEY=your_openai_key_here
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o-mini

# === Azure OpenAI ===
# AZURE_OPENAI_API_KEY=your_azure_key_here
# LLM_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# LLM_MODEL=gpt-4o

# === Local Ollama ===
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2

# === Custom Provider ===
# LLM_BASE_URL=https://your-custom-endpoint.com/v1
# LLM_API_KEY=your_custom_key
# LLM_MODEL=your_model_name

# Story Generation Settings
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000

# Legacy environment variables (still supported)
# STORY_MODEL=gpt-4o-mini
# STORY_TEMPERATURE=0.7  
# MAX_STORY_TOKENS=1000