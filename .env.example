# LLM Configuration - Choose your provider

# === Azure AI Foundry ===
LLM_API_KEY=your_azure_key_here
LLM_BASE_URL=https://your-resource.openai.azure.com
LLM_MODEL=gpt-4o
LLM_API_VERSION=2025-01-01-preview

# === OpenAI API (Direct) ===
# OPENAI_API_KEY=your_openai_key_here
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o

# === Local Ollama ===
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2

# === GitHub Models (Free tier available but limited and may only work with smaller repos) ===
# GITHUB_TOKEN=your_github_token_here
# LLM_BASE_URL=https://models.inference.ai.azure.com
# LLM_MODEL=gpt-4o

# LLM Parameters (Optional - defaults provided)
LLM_TEMPERATURE=1.0      # Temperature for LLM requests (0.1-2.0, default: 1.0, GPT-5 uses 1.0 regardless)
LLM_MAX_TOKENS=10000      # Default max tokens for LLM responses (default: 8000)

# GPT-5 Specific Parameters (Optional - only used for gpt-5, gpt-5-mini models)
GPT5_VERBOSITY=medium         # Response verbosity: low/medium/high (default: medium)
GPT5_REASONING_EFFORT=minimal  # Reasoning effort: minimal/medium/high (default: medium)

# Caching Settings
REPOMIX_CACHE_TTL=60000  # Repomix cache TTL in milliseconds (default: 60 seconds)