# LLM Configuration - Choose your provider

# === Azure AI Foundry ===
LLM_API_KEY=your_azure_key_here
LLM_BASE_URL=https://your-resource.openai.azure.com
LLM_MODEL=gpt-4o
LLM_API_VERSION=2025-01-01-preview

# === OpenAI API ===
# OPENAI_API_KEY=your_openai_key_here
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o

# === Ollama (limited but can be used to experiment)===
# If you use this you'll probably need to extend the LLM_REQUEST_TIMEOUT value significantly
# 300000 (5 minutes) or more depending upon your hardware
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=gemma3:27b

# === GitHub Models (Free tier available but VERY limited and will only work with very small scenarios) ===
# GITHUB_TOKEN=your_github_token_here
# LLM_BASE_URL=https://models.inference.ai.azure.com
# LLM_MODEL=gpt-4o

# LLM Parameters (Optional - defaults provided)
LLM_TEMPERATURE=1.0      # Temperature for LLM requests (0.1-2.0, default: 1.0, GPT-5 uses 1.0 regardless)
LLM_MAX_TOKENS=10000      # Default max tokens for LLM responses (default: 8000)

# GPT-5 Specific Parameters (Optional - only used for gpt-5, gpt-5-mini models)
GPT5_VERBOSITY=medium         # Response verbosity: low/medium/high (default: medium)
GPT5_REASONING_EFFORT=minimal  # Reasoning effort: minimal/medium/high (default: medium)

# Timeout Settings (Optional - defaults provided)
LLM_REQUEST_TIMEOUT=60000        # LLM request timeout in milliseconds (default: 60 seconds, increase for slow models like Ollama)
FILE_READ_TIMEOUT=10000          # File read timeout in milliseconds (default: 10 seconds)
FILE_ANALYSIS_TIMEOUT=30000      # File analysis timeout in milliseconds (default: 30 seconds)
LLM_CACHE_TTL=300000            # LLM cache TTL in milliseconds (default: 5 minutes)
REPOMIX_CACHE_TTL=60000         # Repomix cache TTL in milliseconds (default: 60 seconds)
REPOMIX_SUBPROCESS_TIMEOUT=120000 # Repomix subprocess timeout in milliseconds (default: 2 minutes)
REPOMIX_GRACEFUL_TIMEOUT=5000    # Repomix graceful shutdown timeout in milliseconds (default: 5 seconds)
REPOMIX_MAX_BUFFER_SIZE=104857600 # Repomix max buffer size in bytes (default: 100MB)

# Token Management and Chunking Configuration (Optional - defaults provided)
MAX_CONTEXT_TOKENS=128000              # Maximum context window tokens (default: 128000)
ESTIMATED_TOKENS_PER_CHAR=0.25         # Estimated tokens per character (default: 0.25)
CHUNKING_RESPONSE_TOKENS=10000         # Reserved tokens for LLM response (default: 10000)
CHUNKING_PROMPT_TOKENS=3000            # Reserved tokens for prompt template (default: 3000)
CHUNKING_CONTEXT_SUMMARY_TOKENS=8000   # Reserved tokens for context summary (default: 8000)

# LLM Client Throttling Configuration (Optional - defaults provided)
LLM_INITIAL_THROTTLE_DELAY=1000   # Initial throttle delay in ms (default: 1000)
LLM_MAX_THROTTLE_DELAY=30000      # Maximum throttle delay in ms (default: 30000)
LLM_THROTTLE_DECAY_RATE=0.8       # Rate to reduce delay on success (default: 0.8)

# Rate Limit Configuration (Optional - defaults provided)
TOKEN_RATE_WINDOW_SECONDS=60      # Token rate limit window for Azure S0 tier in seconds (default: 60)

# Input Validation Limits (Optional - defaults provided)
VALIDATION_MAX_CHOICE_LENGTH=200    # Maximum choice input length (default: 200)
VALIDATION_MAX_PATH_LENGTH=500      # Maximum path input length (default: 500)
VALIDATION_MAX_DISPLAY_LENGTH=1000  # Maximum display length (default: 1000)