# LLM Configuration - Choose your provider

# === Azure AI Foundry ===
LLM_API_KEY=your_azure_key_here
LLM_BASE_URL=https://your-resource.openai.azure.com
LLM_MODEL=gpt-4o
LLM_API_VERSION=2025-01-01-preview

# === OpenAI API ===
# OPENAI_API_KEY=your_openai_key_here
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o

# === Ollama (limited but can be used to experiment)===
# If you use this you'll probably need to extend the LLM_REQUEST_TIMEOUT value significantly
# 300000 (5 minutes) or more depending upon your hardware
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=gemma3:27b

# === GitHub Models (Free tier available but VERY limited and will only work with small scenarios) ===
# GITHUB_TOKEN=your_github_token_here
# LLM_BASE_URL=https://models.inference.ai.azure.com
# LLM_MODEL=gpt-4o

# LLM Parameters (Optional - defaults provided)
LLM_TEMPERATURE=1.0      # Temperature for LLM requests (0.1-2.0, default: 1.0, GPT-5 uses 1.0 regardless)
LLM_MAX_TOKENS=10000      # Default max tokens for LLM responses (default: 8000)

# GPT-5 Specific Parameters (Optional - only used for gpt-5, gpt-5-mini models)
GPT5_VERBOSITY=medium         # Response verbosity: low/medium/high (default: medium)
GPT5_REASONING_EFFORT=minimal  # Reasoning effort: minimal/medium/high (default: medium)

# Timeout Settings
LLM_REQUEST_TIMEOUT=60000 # LLM request timeout in milliseconds (default: 60 seconds, increase for slow models like Ollama)
REPOMIX_CACHE_TTL=60000   # Repomix cache TTL in milliseconds (default: 60 seconds)