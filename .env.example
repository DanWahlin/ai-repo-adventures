# LLM Configuration - Choose your provider

# === GitHub Models (Free tier available) ===
GITHUB_TOKEN=your_github_token_here
LLM_BASE_URL=https://models.inference.ai.azure.com
LLM_MODEL=gpt-4o
# Available models: gpt-4o, gpt-4o-mini

# === OpenAI API (Direct) ===
# OPENAI_API_KEY=your_openai_key_here
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o

# === Azure OpenAI ===
# AZURE_OPENAI_API_KEY=your_azure_key_here
# LLM_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# LLM_MODEL=gpt-4o

# === Local Ollama ===
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2

# === Custom Provider ===
# LLM_BASE_URL=https://your-custom-endpoint.com/v1
# LLM_API_KEY=your_custom_key
# LLM_MODEL=your_model_name

# Caching Settings
REPOMIX_CACHE_TTL=60000  # Repomix cache TTL in milliseconds (default: 60 seconds)