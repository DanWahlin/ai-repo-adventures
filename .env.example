# LLM Configuration - Choose your provider

# === GitHub Models (Free tier available) ===
GITHUB_TOKEN=your_github_token_here
LLM_BASE_URL=https://models.inference.ai.azure.com
LLM_MODEL=gpt-4o
# Available models: gpt-4o, gpt-4o-mini

# === OpenAI API (Direct) ===
# OPENAI_API_KEY=your_openai_key_here
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o

# === Azure OpenAI ===
# LLM_API_KEY=your_azure_key_here
# LLM_BASE_URL=https://your-resource.openai.azure.com
# LLM_MODEL=gpt-4o
# LLM_API_VERSION=2025-01-01-preview

# === Azure Cognitive Services ===
# LLM_API_KEY=your_cognitive_services_key_here
# LLM_BASE_URL=https://your-resource.cognitiveservices.azure.com
# LLM_MODEL=gpt-5-mini
# LLM_API_VERSION=2025-01-01-preview

# === Local Ollama ===
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2

# === Custom Provider ===
# LLM_BASE_URL=https://your-custom-endpoint.com/v1
# LLM_API_KEY=your_custom_key
# LLM_MODEL=your_model_name

# LLM Parameters (Optional - defaults provided)
LLM_TEMPERATURE=0.7      # Temperature for LLM requests (0.1-2.0, default: 0.7, GPT-5 uses 1.0 regardless)
LLM_MAX_TOKENS=4000      # Default max tokens for LLM responses (default: 4000)

# GPT-5 Specific Parameters (Optional - only used for gpt-5, gpt-5-mini models)
GPT5_VERBOSITY=medium         # Response verbosity: low/medium/high (default: medium)
GPT5_REASONING_EFFORT=medium  # Reasoning effort: minimal/medium/high (default: medium)

# Caching Settings
REPOMIX_CACHE_TTL=60000  # Repomix cache TTL in milliseconds (default: 60 seconds)