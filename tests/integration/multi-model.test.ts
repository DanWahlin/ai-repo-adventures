#!/usr/bin/env node

/**
 * Multi-Model Integration Tests
 * Tests that different LLM models generate compatible output
 * Only runs if LLM_MULTI_MODEL_TESTS=true (expensive)
 */

import { resolve } from 'path';
import { existsSync } from 'fs';
import { AdventureManager } from '../../packages/core/dist/adventure/adventure-manager.js';
import { LLMClient } from '../../packages/core/dist/llm/llm-client.js';
import { RepoAnalyzer } from '../../packages/core/dist/analyzer/repo-analyzer.js';
import { createProjectInfo } from '../../packages/core/dist/index.js';
import {
  validateQuestResponse,
  validateStoryResponse
} from '../../packages/core/dist/shared/llm-response-validator.js';
import { LLM_TEST_MODELS, LLM_MULTI_MODEL_TESTS } from '../../packages/core/dist/shared/config.js';
import { createTestRunner, assert, TestHelpers } from '../shared/test-utils.js';

/**
 * Use the actual ai-repo-adventures repository for testing
 * This provides realistic data with a real adventure.config.json
 */
function getTestProjectPath(): string {
  // Get the root directory of this repository
  const repoRoot = resolve(process.cwd());

  // Verify adventure config exists
  const configPath = resolve(repoRoot, 'adventure.config.json');
  if (!existsSync(configPath)) {
    throw new Error(`adventure.config.json not found at ${configPath}`);
  }

  console.log(`ðŸ“ Using actual repository at: ${repoRoot}`);
  console.log(`âœ… Found adventure.config.json with real quest definitions\n`);

  return repoRoot;
}

/**
 * Generate real project info by analyzing the actual repository
 */
async function generateRealProjectInfo(projectPath: string) {
  console.log('ðŸ” Analyzing repository (this may take a moment)...');
  const analyzer = new RepoAnalyzer();
  const repomixContent = await analyzer.generateRepomixContext(projectPath);
  const projectInfo = createProjectInfo(repomixContent);
  console.log(`âœ… Generated ${repomixContent.length} chars of repository context\n`);
  return projectInfo;
}

async function runTests() {
  console.log('ðŸ¤– Running Multi-Model Integration Tests');
  console.log('');

  // Check if multi-model tests are enabled
  if (!LLM_MULTI_MODEL_TESTS) {
    console.log('â­ï¸  Multi-model tests are DISABLED (LLM_MULTI_MODEL_TESTS=false)');
    console.log('ðŸ’¡ Set LLM_MULTI_MODEL_TESTS=true in .env to enable these tests');
    console.log('âš ï¸  Note: Multi-model tests are expensive and make multiple LLM API calls\n');
    console.log('âœ… Skipping multi-model tests (this is expected behavior)');
    process.exit(0);
  }

  console.log(`ðŸŽ¯ Testing ${LLM_TEST_MODELS.length} model(s): ${LLM_TEST_MODELS.join(', ')}`);
  console.log('âš ï¸  This will make multiple API calls and may take several minutes');
  console.log('');

  // Use the actual ai-repo-adventures repository
  const testProjectPath = getTestProjectPath();

  // Generate real project info from actual repository
  const realProjectInfo = await generateRealProjectInfo(testProjectPath);

  const { test, stats, printResults } = await createTestRunner('Multi-Model Integration Tests');

  // Model Response Structure Tests
  console.log('ðŸ“‹ Model Response Structure Tests');
  console.log('-'.repeat(40));

  await test('All models generate valid story structure', async () => {
    const results = new Map<string, any>();

    for (const model of LLM_TEST_MODELS) {
      console.log(`\n  Testing model: ${model}`);

      // Create story generator with specific model's LLM client
      const llmClient = new LLMClient({ model });
      const storyGenerator = new (await import('../../packages/core/dist/adventure/story-generator.js')).StoryGenerator();

      // Override the story generator's LLM client
      (storyGenerator as any).llmClient = llmClient;

      // Generate raw story markdown (matches what HTML generator actually parses)
      const storyResponse = await storyGenerator.generateStoryAndQuests(realProjectInfo, 'space', testProjectPath);

      // Reconstruct the full markdown as it would be generated by the LLM
      const rawMarkdown = `# ${storyResponse.title}\n\n## Story\n${storyResponse.story}\n\n## Choose a Quest\n\n` +
        storyResponse.quests.map((q, i) => `${i+1}. **${q.title}** â€“ ${q.description}`).join('\n\n');

      // Validate the raw markdown structure
      const validation = validateStoryResponse(rawMarkdown);

      results.set(model, {
        story: rawMarkdown,
        validation,
        length: rawMarkdown.length
      });

      console.log(`    Story length: ${rawMarkdown.length} chars`);
      console.log(`    Valid: ${validation.valid ? 'âœ…' : 'âŒ'}`);

      if (!validation.valid) {
        console.log(`    Errors: ${validation.errors.join(', ')}`);
      }

      // Assert each model produces valid output
      assert(validation.valid === true, `Model ${model} should generate valid story structure`);
      assert(rawMarkdown.length > 200, `Model ${model} story should be substantial`);
    }

    // Compare outputs if testing multiple models
    if (LLM_TEST_MODELS.length > 1) {
      console.log(`\n  ðŸ“Š Comparing ${LLM_TEST_MODELS.length} model outputs:`);

      const lengths = Array.from(results.values()).map(r => r.length);
      const avgLength = lengths.reduce((a, b) => a + b, 0) / lengths.length;
      const maxLength = Math.max(...lengths);
      const minLength = Math.min(...lengths);

      console.log(`    Average length: ${Math.round(avgLength)} chars`);
      console.log(`    Range: ${minLength} - ${maxLength} chars`);

      // All models should produce reasonably similar lengths (within 3x)
      assert(
        maxLength / minLength < 3,
        `Model outputs should be similar in length (max/min ratio: ${(maxLength / minLength).toFixed(2)})`
      );
    }
  }, { timeout: 180000 }); // 3 minutes per model test

  await test('All models generate valid quest content', async () => {
    const results = new Map<string, any>();

    for (const model of LLM_TEST_MODELS) {
      console.log(`\n  Testing model: ${model}`);

      const llmClient = new LLMClient({ model });
      const manager = new AdventureManager();
      (manager as any).llmClient = llmClient;

      // Initialize and explore first quest with project path
      await manager.initializeAdventure(realProjectInfo, 'mythical', testProjectPath);
      const quest = await manager.exploreQuest('1');

      // Validate structure
      const validation = validateQuestResponse(quest.narrative);

      results.set(model, {
        quest: quest.narrative,
        validation,
        length: quest.narrative.length
      });

      console.log(`    Quest length: ${quest.narrative.length} chars`);
      console.log(`    Valid: ${validation.valid ? 'âœ…' : 'âŒ'}`);

      if (!validation.valid) {
        console.log(`    Errors: ${validation.errors.join(', ')}`);
      }

      // Assert validity
      assert(validation.valid === true, `Model ${model} should generate valid quest structure`);
      assert(quest.narrative.length > 500, `Model ${model} quest should be detailed`);
      assert(quest.completed === true, `Model ${model} quest should be marked completed`);
    }

    if (LLM_TEST_MODELS.length > 1) {
      console.log(`\n  ðŸ“Š Quest content comparison:`);

      const lengths = Array.from(results.values()).map(r => r.length);
      const avgLength = lengths.reduce((a, b) => a + b, 0) / lengths.length;

      console.log(`    Average quest length: ${Math.round(avgLength)} chars`);
    }
  }, { timeout: 180000 });

  // Theme Consistency Tests
  console.log('');
  console.log('ðŸŽ¨ Theme Consistency Tests');
  console.log('-'.repeat(40));

  await test('All models respect theme guidelines', async () => {
    const themes: Array<'space' | 'ancient' | 'mythical'> = ['space', 'ancient', 'mythical'];

    for (const theme of themes) {
      console.log(`\n  Testing theme: ${theme}`);

      for (const model of LLM_TEST_MODELS) {
        console.log(`    Model: ${model}`);

        const llmClient = new LLMClient({ model });
        const storyGenerator = new (await import('../../packages/core/dist/adventure/story-generator.js')).StoryGenerator();
        (storyGenerator as any).llmClient = llmClient;

        const storyResponse = await storyGenerator.generateStoryAndQuests(realProjectInfo, theme, testProjectPath);
        const story = `# ${storyResponse.title}\n\n## Story\n${storyResponse.story}\n\n## Choose a Quest\n\n` +
          storyResponse.quests.map((q, i) => `${i+1}. **${q.title}** â€“ ${q.description}`).join('\n\n');

        // Check for theme-appropriate vocabulary
        const themeWords = {
          space: ['space', 'cosmic', 'star', 'galaxy', 'ship', 'stellar', 'orbit'],
          ancient: ['temple', 'ancient', 'wisdom', 'sacred', 'priest', 'ritual'],
          mythical: ['quest', 'kingdom', 'castle', 'knight', 'magic', 'realm']
        };

        const expectedWords = themeWords[theme];
        const foundWords = TestHelpers.getFoundWords(story, expectedWords);

        console.log(`      Theme words found: ${foundWords.join(', ')}`);

        assert(
          foundWords.length > 0,
          `Model ${model} should use ${theme} theme vocabulary (expected one of: ${expectedWords.join(', ')})`
        );

        // Validate structure
        const validation = validateStoryResponse(story);
        assert(
          validation.valid === true,
          `Model ${model} should generate valid ${theme} story`
        );
      }
    }
  }, { timeout: 300000 }); // 5 minutes for all theme/model combinations

  // Output Format Consistency Tests
  console.log('');
  console.log('ðŸ“ Output Format Consistency Tests');
  console.log('-'.repeat(40));

  await test('All models use consistent markdown formatting', async () => {
    const formatChecks = {
      hasH1: false,
      hasH2: false,
      hasCodeBlocks: false,
      hasLists: false
    };

    for (const model of LLM_TEST_MODELS) {
      console.log(`\n  Checking format for model: ${model}`);

      const llmClient = new LLMClient({ model });
      const manager = new AdventureManager();
      (manager as any).llmClient = llmClient;

      await manager.initializeAdventure(realProjectInfo, 'space', testProjectPath);
      const quest = await manager.exploreQuest('1');

      // Check markdown elements
      const hasH1 = /^#\s+/m.test(quest.narrative);
      const hasH2 = /^##\s+/m.test(quest.narrative);
      const hasCodeBlocks = /```/.test(quest.narrative);
      const hasLists = /^[\*\-\d]\./m.test(quest.narrative);

      console.log(`    H1 headings: ${hasH1 ? 'âœ…' : 'âŒ'}`);
      console.log(`    H2 headings: ${hasH2 ? 'âœ…' : 'âŒ'}`);
      console.log(`    Code blocks: ${hasCodeBlocks ? 'âœ…' : 'âŒ'}`);
      console.log(`    Lists: ${hasLists ? 'âœ…' : 'âŒ'}`);

      // All models should use these basic markdown features
      assert(hasH1 === true, `Model ${model} should use H1 heading for quest title`);
      assert(hasH2 === true, `Model ${model} should use H2 headings for sections`);
    }
  }, { timeout: 180000 });

  // Error Recovery Tests
  console.log('');
  console.log('ðŸ”§ Error Recovery Tests');
  console.log('-'.repeat(40));

  await test('All models handle minimal context gracefully', async () => {
    const minimalProject = {
      ...realProjectInfo,
      repomixContent: '# Test Project\n\nMinimal content for testing.\n\nfile: test.ts'
    };

    for (const model of LLM_TEST_MODELS) {
      console.log(`\n  Testing model: ${model} with minimal context`);

      const llmClient = new LLMClient({ model });
      const storyGenerator = new (await import('../../packages/core/dist/adventure/story-generator.js')).StoryGenerator();
      (storyGenerator as any).llmClient = llmClient;

      try {
        const storyResponse = await storyGenerator.generateStoryAndQuests(minimalProject, 'developer', testProjectPath);
        const story = `# ${storyResponse.title}\n\n## Story\n${storyResponse.story}\n\n## Choose a Quest\n\n` +
          storyResponse.quests.map((q, i) => `${i+1}. **${q.title}** â€“ ${q.description}`).join('\n\n');

        // Should still generate something valid
        assert(story.length > 100, `Model ${model} should generate content even with minimal input`);

        const validation = validateStoryResponse(story);
        console.log(`    Generated ${story.length} chars, valid: ${validation.valid ? 'âœ…' : 'âŒ'}`);

        if (!validation.valid) {
          console.log(`    Errors: ${validation.errors.join(', ')}`);
        }
      } catch (error) {
        // It's okay if models struggle with minimal context, but they shouldn't crash
        console.log(`    Model handled minimal context with error: ${error instanceof Error ? error.message : String(error)}`);
        assert(true, 'Model should handle errors gracefully');
      }
    }
  }, { timeout: 120000 });

  printResults();

  if (stats.failed > 0) {
    console.log('\nâš ï¸  Some models produced incompatible output');
    console.log('ðŸ’¡ Consider updating prompts or validation rules');
    process.exit(1);
  } else {
    console.log('\nâœ… All models produce compatible output!');
    console.log('ðŸŽ‰ The system works across different LLM models');
    process.exit(0);
  }
}

if (import.meta.url === `file://${process.argv[1]}`) {
  runTests().catch(error => {
    console.error('ðŸ’¥ Multi-model test runner failed:', error);
    process.exit(1);
  });
}

export { runTests };